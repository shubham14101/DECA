{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38, 1, 8000])\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "import torchaudio\n",
    "sound, sample_rate = torchaudio.load('./dataset/audios/s1_u1.wav')\n",
    "\n",
    "sound_avg = sound.mean(1)\n",
    "audio_len = sound_avg.size()[0]\n",
    "sample_window = 8000\n",
    "overlapping = 2000\n",
    "left = 0\n",
    "right = 8000\n",
    "audio_data = torch.Tensor()\n",
    "while left < audio_len:\n",
    "    sample = sound_avg[left:right]\n",
    "    if sample.size()[0] != 8000:\n",
    "        break\n",
    "    sample = sample.view(1, 1, sample.size()[0])\n",
    "    audio_data = torch.cat((audio_data, sample))\n",
    "#     print(audio_data.size())\n",
    "#     print(sample.size())\n",
    "    left = right - overlapping\n",
    "    right = left + sample_window\n",
    "\n",
    "\n",
    "    \n",
    "print(audio_data.size())\n",
    "sound_inp = sound_avg.view(1, 1, sound_avg.size()[0])\n",
    "# overlappig = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38, 1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 1, 256])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Context Encoder\n",
    "\n",
    "class Conv1DAudio(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel, stride, padding):\n",
    "        super(Conv1DAudio, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_ch, out_channels=out_ch, kernel_size=kernel, stride=stride, padding=padding),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioEncoder, self).__init__()\n",
    "        self.a_c_1 = Conv1DAudio(1, 16, 250, 50, 100)\n",
    "        self.a_c_2 = Conv1DAudio(16, 32, 4, 2, 1)\n",
    "        self.a_c_3 = Conv1DAudio(32, 64, 4, 2, 1)\n",
    "        self.a_c_4 = Conv1DAudio(64, 128, 4, 2, 1)\n",
    "        self.a_c_5 = Conv1DAudio(128, 256, 4, 2, 1)\n",
    "        self.a_c_6 = Conv1DAudio(256, 512, 4, 2, 1)\n",
    "        self.fc1 = nn.Linear(5*512, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ac1 = self.a_c_1(x)    \n",
    "        ac2 = self.a_c_2(ac1)\n",
    "        ac3 = self.a_c_3(ac2)\n",
    "        ac4 = self.a_c_4(ac3)\n",
    "        ac5 = self.a_c_5(ac4)\n",
    "        ac6 = self.a_c_6(ac5)\n",
    "        ac6 = ac6.view(-1, 5*512)\n",
    "        audio_encoding = self.fc1(ac6)\n",
    "        return audio_encoding\n",
    "\n",
    "# audio_encoder = AudioEncoder()\n",
    "# encoded = audio_encoder(data)\n",
    "\n",
    "\n",
    "audio_enc = AudioEncoder()\n",
    "out = audio_enc(audio_data)\n",
    "out = out.view(out.size()[0], 1, out.size()[1])\n",
    "print(out.size())\n",
    "\n",
    "SEQ_LEN = 1\n",
    "AUDIO_OUTPUT = 256\n",
    "BATCH = 100\n",
    "HIDDEN_SIZE_AUDIO = 256\n",
    "NUM_LAYERS_AUDIO = 2\n",
    "\n",
    "# input = torch.randn(SEQ_LEN, BATCH, AUDIO_OUTPUT)\n",
    "rnn = nn.GRU(AUDIO_OUTPUT, HIDDEN_SIZE_AUDIO, NUM_LAYERS_AUDIO)\n",
    "output, hn = rnn(out)\n",
    "output.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 1, 10])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noise Encoder\n",
    "\n",
    "SEQ_LEN = 1\n",
    "NOISE_OUTPUT = 10\n",
    "BATCH = 38\n",
    "HIDDEN_SIZE_NOISE = 10\n",
    "NUM_LAYERS_NOISE = 1\n",
    "noise = torch.rand(BATCH, 1, NOISE_OUTPUT)\n",
    "noise = Variable(noise)\n",
    "noise = noise.data.resize_(noise.size()).normal_(0, 0.6)\n",
    "rnn = nn.GRU(NOISE_OUTPUT, HIDDEN_SIZE_NOISE, NUM_LAYERS_NOISE)\n",
    "output_noise, hn_noise = rnn(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity Encoder and Frame Decoder\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_x, kernel_y, stride, padding):\n",
    "        super(Down, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=(kernel_x, kernel_y),\n",
    "            stride=stride,\n",
    "            padding=padding),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, d_k_x, d_k_y, s_d, c_k_x, c_k_y, s_c, padding):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(\n",
    "            in_channels=in_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=(d_k_x, d_k_y),\n",
    "            stride=s_d\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=2*out_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=(c_k_x, c_k_y),\n",
    "            stride=s_c,\n",
    "            padding=padding\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        print(\"x1 size \", x1.size())\n",
    "        print(\"x2 size \", x2.size())\n",
    "        x1 = self.deconv(x1)\n",
    "        diffX = x1.size()[2] - x2.size()[2]\n",
    "        diffY = x1.size()[3] - x2.size()[3]\n",
    "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
    "                        diffY // 2, int(diffY / 2)))\n",
    "        print(\"x1 after deconv size \", x1.size())\n",
    "        print(\"x2 after pad size \", x2.size())\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "\n",
    "#         x = torch.add(x1, x2)\n",
    "        print(\"x size \", x.size())\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        \n",
    "        # Identity Encoder \n",
    "        \n",
    "        self.i_c_1 = Down(3, 64, 4, 4, 2, 1)\n",
    "        self.i_c_2 = Down(64, 128, 4, 4, 2, 1)\n",
    "        self.i_c_3 = Down(128, 256, 4, 4, 2, 1)\n",
    "        self.i_c_4 = Down(256, 512, 4, 4, 2, 1)\n",
    "        self.i_c_5 = Down(512, 1024, 4, 4, 2, 1)\n",
    "        self.i_c_6 = Down(1024, 50, 3, 3, 2, 1)\n",
    "        \n",
    "        # Frame decoder\n",
    "        \n",
    "        self.f_d_1 = Up(50, 1024, 4, 4, 1, 3, 3, 2, 1)\n",
    "        self.f_d_2 = Up(1024, 512, 4, 4, 2, 3, 3, 1, 1)\n",
    "        self.f_d_3 = Up(512, 256, 4, 4, 2, 3, 3, 1, 4)\n",
    "        self.f_d_4 = Up(256, 128, 4, 4, 2, 3, 3, 1, 4)\n",
    "        \n",
    "        self.f_d_5 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(\n",
    "            in_channels=128,\n",
    "            out_channels=64, \n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=5,\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.f_d_6 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(\n",
    "            in_channels=64,\n",
    "            out_channels=3, \n",
    "            kernel_size=(3,3),\n",
    "            stride=2,\n",
    "            padding=1\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        i_64 = self.i_c_1(x)\n",
    "        print(i_64.size())\n",
    "        i_128 = self.i_c_2(i_64)\n",
    "        print(i_128.size())\n",
    "        i_256 = self.i_c_3(i_128)\n",
    "        print(i_256.size())\n",
    "        i_512 = self.i_c_4(i_256)\n",
    "        print(i_512.size())\n",
    "        i_1024 = self.i_c_5(i_512)\n",
    "        print(i_1024.size())\n",
    "        \n",
    "        # concatenate noise and audio\n",
    "        latent = self.i_c_6(i_1024)\n",
    "        print(latent.size())\n",
    "        print(\"---------------------\")\n",
    "        f_1024 = self.f_d_1(latent, i_1024)\n",
    "        print('f 1024 ', f_1024.size())\n",
    "        f_512 = self.f_d_2(f_1024, i_512)\n",
    "        print('f 512 ', f_512.size())\n",
    "        f_256 = self.f_d_3(f_512, i_256)\n",
    "        print('f 256 ', f_256.size())\n",
    "        f_128 = self.f_d_4(f_256, i_128)\n",
    "        print('f 128 ', f_128.size())\n",
    "        f_64 = self.f_d_5(f_128)\n",
    "        print('f 64 ', f_64.size())\n",
    "        f_3 = self.f_d_6(f_64)\n",
    "        print('f 3 ', f_3.size())\n",
    "        return f_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 96, 96])\n",
      "torch.Size([1, 64, 48, 48])\n",
      "torch.Size([1, 128, 24, 24])\n",
      "torch.Size([1, 256, 12, 12])\n",
      "torch.Size([1, 512, 6, 6])\n",
      "torch.Size([1, 1024, 3, 3])\n",
      "torch.Size([1, 50, 2, 2])\n",
      "---------------------\n",
      "x1 size  torch.Size([1, 50, 2, 2])\n",
      "x2 size  torch.Size([1, 1024, 3, 3])\n",
      "x1 after deconv size  torch.Size([1, 1024, 5, 5])\n",
      "x2 after pad size  torch.Size([1, 1024, 5, 5])\n",
      "x size  torch.Size([1, 2048, 5, 5])\n",
      "f 1024  torch.Size([1, 1024, 3, 3])\n",
      "x1 size  torch.Size([1, 1024, 3, 3])\n",
      "x2 size  torch.Size([1, 512, 6, 6])\n",
      "x1 after deconv size  torch.Size([1, 512, 8, 8])\n",
      "x2 after pad size  torch.Size([1, 512, 8, 8])\n",
      "x size  torch.Size([1, 1024, 8, 8])\n",
      "f 512  torch.Size([1, 512, 8, 8])\n",
      "x1 size  torch.Size([1, 512, 8, 8])\n",
      "x2 size  torch.Size([1, 256, 12, 12])\n",
      "x1 after deconv size  torch.Size([1, 256, 18, 18])\n",
      "x2 after pad size  torch.Size([1, 256, 18, 18])\n",
      "x size  torch.Size([1, 512, 18, 18])\n",
      "f 256  torch.Size([1, 256, 24, 24])\n",
      "x1 size  torch.Size([1, 256, 24, 24])\n",
      "x2 size  torch.Size([1, 128, 24, 24])\n",
      "x1 after deconv size  torch.Size([1, 128, 50, 50])\n",
      "x2 after pad size  torch.Size([1, 128, 50, 50])\n",
      "x size  torch.Size([1, 256, 50, 50])\n",
      "f 128  torch.Size([1, 128, 56, 56])\n",
      "f 64  torch.Size([1, 64, 48, 48])\n",
      "f 3  torch.Size([1, 3, 95, 95])\n"
     ]
    }
   ],
   "source": [
    "data_transform = {\n",
    "        'frames': transforms.Compose([\n",
    "            transforms.Resize(96),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "frames = datasets.ImageFolder('./dataset/', data_transform['frames'])\n",
    "dset_loader = torch.utils.data.DataLoader(frames, batch_size=1, shuffle=False, num_workers=4)\n",
    "model = Unet()\n",
    "# out = model()\n",
    "for data in dset_loader:\n",
    "    inputs, _ = data\n",
    "print(inputs.size())\n",
    "image = torch.rand(1, 3, 96, 96)\n",
    "out = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14c4e7e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_image(inp):\n",
    "    \"\"\"\n",
    "        See a particular image\n",
    "    \"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    plt.imshow(inp)\n",
    "#     plt.show()\n",
    "    \n",
    "visualize_image(inputs[0])\n",
    "# # inputs[0].size()\n",
    "# print(inputs.size())\n",
    "# print(type(out[0].detach().numpy()))\n",
    "visualize_image(out[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 57, 57])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8000])\n",
      "torch.Size([1, 3, 96, 96])\n",
      "torch.Size([1, 64, 47, 47])\n",
      "torch.Size([1, 128, 23, 23])\n",
      "torch.Size([1, 256, 11, 11])\n",
      "torch.Size([1, 512, 5, 5])\n",
      "torch.Size([1, 256, 3, 3])\n",
      "o_i  torch.Size([1, 50, 1, 1])\n",
      "o_i after GRU torch.Size([1, 1, 50])\n",
      "Audio Encoder\n",
      "o_a  torch.Size([1, 256])\n",
      "o_a after GRU torch.Size([1, 1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4797,  0.5142]]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discriminator Models\n",
    "\n",
    "class Convolution(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel, stride):\n",
    "        super(Convolution, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_ch, \n",
    "                out_channels=out_ch, \n",
    "                kernel_size=kernel, \n",
    "                stride=stride), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "class FrameDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrameDiscriminator, self).__init__()\n",
    "        \n",
    "        self.c1 = Convolution(3, 64, 4, 2)\n",
    "        self.c2 = Convolution(64, 128, 4, 2)\n",
    "        self.c3 = Convolution(128, 256, 4, 2)\n",
    "        self.c4 = Convolution(256, 512, 4, 2)\n",
    "        self.c5 = Convolution(512, 1024, 4, 2)\n",
    "        self.c6 = nn.Linear(1024*4*4, 128)\n",
    "        self.c7 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.c5(x)\n",
    "        x = x.view(-1, 1024*4*4)\n",
    "        x = self.c6(x)\n",
    "        out = self.c7(x)\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.i_c_1 = Convolution(3, 64, 3, 2)\n",
    "        self.i_c_2 = Convolution(64, 128, 3, 2)\n",
    "        self.i_c_3 = Convolution(128, 256, 3, 2)\n",
    "        self.i_c_4 = Convolution(256, 512, 3, 2)\n",
    "#         self.i_c_5 = Convolution(512, 1024, 4, 2)\n",
    "        self.i_c_5 = Convolution(512, 256, 3, 1)\n",
    "        self.i_c_6 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=256, \n",
    "                out_channels=50, \n",
    "                kernel_size=(3,3), \n",
    "                stride=2), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        i_64 = self.i_c_1(x)\n",
    "        print(i_64.size())\n",
    "        i_128 = self.i_c_2(i_64)\n",
    "        print(i_128.size())\n",
    "        i_256 = self.i_c_3(i_128)\n",
    "        print(i_256.size())\n",
    "        i_512 = self.i_c_4(i_256)\n",
    "        print(i_512.size())\n",
    "        i_1024 = self.i_c_5(i_512)\n",
    "        print(i_1024.size())\n",
    "        latent = self.i_c_6(i_1024) \n",
    "        return latent\n",
    "\n",
    "\n",
    "class SequenceDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SequenceDiscriminator, self).__init__()\n",
    "        self.audio_encoder = AudioEncoder()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.gru_image = nn.GRU(50, 50, 2)\n",
    "        self.gru_audio = nn.GRU(256, 256, 2)\n",
    "        self.fc1 = nn.Linear(306, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, i_i, i_a):\n",
    "        o_i = self.image_encoder(i_i)\n",
    "        print(\"o_i \", o_i.size())\n",
    "        o_i = o_i.view(1, 1, 50)\n",
    "        o_i, h_n = self.gru_image(o_i)\n",
    "        print(\"o_i after GRU\", o_i.size())\n",
    "        print(\"Audio Encoder\")\n",
    "        o_a = self.audio_encoder(i_a)\n",
    "        print(\"o_a \", o_a.size())\n",
    "        o_a = o_a.view(1, 1, 256)        \n",
    "        o_a, h_n = self.gru_audio(o_a)\n",
    "\n",
    "        print(\"o_a after GRU\", o_a.size())        \n",
    "        x = torch.cat([o_a, o_i], dim=2)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        out = F.sigmoid(x)\n",
    "        return out   \n",
    "\n",
    "# frame_dis = FrameDiscriminator()\n",
    "# out = frame_dis(inputs)\n",
    "# out\n",
    "\n",
    "# image_enc = ImageEncoder()\n",
    "# out = image_enc(inputs)\n",
    "# out.size()\n",
    "# print(audio_data.size()[2])\n",
    "audio_in = audio_data[0].view(1, 1, audio_data.size()[2])\n",
    "# print(audio_data[0].size())\n",
    "print(audio_in.size())\n",
    "print(inputs.size())\n",
    "seq_dis = SequenceDiscriminator()\n",
    "out = seq_dis(inputs, audio_in)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Tensors must have same number of dimensions: got 3 and 4 at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:3577",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ea7bf562e15a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m38\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m38\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 3 and 4 at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:3577"
     ]
    }
   ],
   "source": [
    "a = torch.rand(38, 1, 256)\n",
    "b = torch.rand(38, 50, 2, 2)\n",
    "c = torch.rand(38, 1, 10)\n",
    "torch.cat((a, b, c), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7765],\n",
      "        [ 0.1586],\n",
      "        [ 0.8227],\n",
      "        [ 0.5306],\n",
      "        [ 0.8655]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4984],\n",
       "        [-0.1727],\n",
       "        [-1.7297],\n",
       "        [-0.7562],\n",
       "        [-2.0060]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = torch.rand(5, 1)\n",
    "print(n)\n",
    "torch.log(1 - n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
